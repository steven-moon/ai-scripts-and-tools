# AI Scripts and Tools - Environment Configuration
# Copy this file to .env and fill in your API keys

# =============================================================================
# LLM PROVIDER SETTINGS
# =============================================================================

# Choose your default LLM provider
# Options: local, openai, anthropic, gemini, custom
LLM_PROVIDER=local

# General LLM settings
LLM_MODEL=
LLM_TEMPERATURE=0.5
LLM_MAX_TOKENS=500

# =============================================================================
# OPENAI SETTINGS
# =============================================================================

# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ENDPOINT=https://api.openai.com/v1/completions
OPENAI_DEFAULT_MODEL=gpt-3.5-turbo-instruct

# Available OpenAI models:
# - gpt-3.5-turbo-instruct
# - gpt-4-turbo-preview
# - gpt-4o-mini

# =============================================================================
# ANTHROPIC SETTINGS
# =============================================================================

# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_ENDPOINT=https://api.anthropic.com/v1/messages
ANTHROPIC_DEFAULT_MODEL=claude-3-sonnet-20240229

# Available Anthropic models:
# - claude-3-sonnet-20240229
# - claude-3-opus-20240229
# - claude-3-haiku-20240307

# =============================================================================
# GOOGLE GEMINI SETTINGS
# =============================================================================

# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_ENDPOINT=https://generativelanguage.googleapis.com/v1
GEMINI_DEFAULT_MODEL=gemini-pro

# Available Gemini models:
# - gemini-pro
# - gemini-1.5-pro
# - gemini-1.5-flash

# =============================================================================
# LOCAL LLM SETTINGS (Ollama, LM Studio, etc.)
# =============================================================================

# Local LLM endpoint (usually Ollama or LM Studio)
LOCAL_LLM_ENDPOINT=http://127.0.0.1:1234/v1/completions
LOCAL_LLM_MODEL=llama-3.2-3b-instruct

# Common local models:
# - llama-3.2-3b-instruct
# - llama-3.1-8b-instruct
# - codellama:7b-instruct

# =============================================================================
# CUSTOM PROVIDER SETTINGS (OpenAI API Compatible)
# =============================================================================

# For any OpenAI-compatible API service
CUSTOM_PROVIDER_NAME=My Custom LLM
CUSTOM_API_KEY=your_custom_api_key_here
CUSTOM_ENDPOINT=https://your-custom-endpoint.com/v1/completions
CUSTOM_DEFAULT_MODEL=your-custom-model-name

# Examples of custom providers:
# - Azure OpenAI Service
# - Self-hosted Ollama with OpenAI compatibility
# - Other commercial LLM services

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Enable debug logging for LLM clients
LLM_DEBUG=false

# Code summary ignore patterns (comma-separated)
CODE_SUMMARY_IGNORE_PATTERNS=*.log,*.tmp,*.bak,temp_*

# =============================================================================
# SHELL INTEGRATION
# =============================================================================

# Path to your ai-scripts-and-tools repository
# Update this to match your actual repository location
AI_SCRIPTS_PATH=/Users/stevenmoon/GitRepo/ai-scripts-and-tools 